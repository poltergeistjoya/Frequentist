{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poltergeistjoya/Frequentist/blob/main/Freqproj3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Project 3: K-fold Validation*** \\\\\n",
        "Joya Debi, Melina Tsai, Sue (Xueru) Zhou\n",
        "\n",
        "*** Summary ***\n",
        "Re-implement the example in section 7.10.2 using any simple, out of the box classifier (like K nearest neighbors from sci-kit). Reproduce the results for the incorrect and correct way of doing cross-validation. \n",
        "\n",
        "In the incorrect way of doing K-fold cross validation on low quality data, accuracy on the validation set was 90-100%. Additionally, the accuracy was relatively the same for each fold of the data. This shows that when doing K-fold cross validation improperly on low quality, randomly generated data, the results can be quite decieving by indicating strong patterns in the data when there are none.\n",
        "\n",
        "When doing K-fold cross validation properly on the same low-quality data, we see the accuracy range from 20% to 70%, with different accuracies for each fold. This is more expected on the dataset."
      ],
      "metadata": {
        "id": "PVN_UCthelkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6hpEiOdd614"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make data 50 x 5000 gaussians\n",
        "#note there is no rng so every time this cell is run, there will be different results\n",
        "data = np.random.normal(0, 0.1,(50,5000))\n",
        "data_df= pd.DataFrame(data)\n",
        "\n",
        "#make labels of 50% 1 and 50% 0 \n",
        "labels = np.random.randint(2, size=50)\n",
        "labels_df =pd.DataFrame(labels)"
      ],
      "metadata": {
        "id": "A-jMSCzPCjq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to return training set with top 100 features and indices of top 100 features\n",
        "\n",
        "def get_top_100(data_df, labels_df, right, tmptrain):\n",
        "  \n",
        "  #1x5000 correlation vector to hold correlation values\n",
        "  correlation = np.zeros(data_df.shape[1])#:5000)\n",
        "\n",
        "  #go through every column in data and correlate with labels\n",
        "  for i in range(0,data_df.shape[1]):\n",
        "    corr = scipy.stats.pearsonr(data_df.iloc[:,i], labels_df)\n",
        "    correlation[i] = corr[0]\n",
        "\n",
        "  #Pearsons coeffients gives values from -1 to 1 so must absolute val\n",
        "  correlation = np.abs(correlation)\n",
        "\n",
        "  #get indices of 100 most correlated\n",
        "  top_100 = np.argsort(correlation)[-100:]\n",
        "\n",
        "  #extract top columns from to make training est\n",
        "  if right == False:\n",
        "    top_100_df = data_df.iloc[np.arange(data_df.shape[0]),top_100]\n",
        "  elif right == True:\n",
        "    top_100_df = tmptrain.iloc[np.arange(tmptrain.shape[0]),top_100]\n",
        "  return top_100_df, top_100"
      ],
      "metadata": {
        "id": "In7mWvzNOuaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Wrong Way ##"
      ],
      "metadata": {
        "id": "sKd9auvC-eYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#in the wrong way, only get top 100 features once, outside the fold\n",
        "top_100_df, top_100 = get_top_100(data_df, labels_df, False, data_df)\n",
        "neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "# make folder\n",
        "nfolds = 5\n",
        "kf = KFold(n_splits=nfolds)\n",
        "for train_index, test_index in kf.split(top_100_df):\n",
        "  train_data = top_100_df.iloc[train_index]\n",
        "  train_labels = labels_df.iloc[train_index]\n",
        "  val_data = top_100_df.iloc[test_index]\n",
        "  val_labels = labels_df.iloc[test_index]\n",
        "  neigh.fit(train_data, train_labels.to_numpy().flatten())\n",
        "  acc = neigh.score(val_data,val_labels)\n",
        "  print(acc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "po4F-PaEnEQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6c7209-6564-48a9-a17b-cbd7cf16c95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.9\n",
            "1.0\n",
            "0.9\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Right Way ##"
      ],
      "metadata": {
        "id": "_oUlPmYs-X_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nfolds = 5\n",
        "kf = KFold(n_splits=nfolds)\n",
        "#right way to split data\n",
        "for train_index, test_index in kf.split(data_df):\n",
        "  #get training set from folds\n",
        "  train_temp = data_df.iloc[train_index]\n",
        "  train_labels = labels_df.iloc[train_index]\n",
        "\n",
        "  #get training set with top 100 features, and indices of those features\n",
        "  r_top_100_df, r_top_100 = get_top_100(train_temp, train_labels, True, train_temp)\n",
        "  train_data = r_top_100_df\n",
        "  neigh.fit(train_data, train_labels.to_numpy().flatten()) #to numpy and flatten for data leakage warning\n",
        "  \n",
        "  #make val data with test indices and top 100 features\n",
        "  val_data = data_df.iloc[test_index, r_top_100]\n",
        "  val_labels = labels_df.iloc[test_index]\n",
        "\n",
        " #two different ways to compute accuracy. Acc1 uses .predict and a separate accuracy score, acc2 uses .score\n",
        "  pred = neigh.predict(val_data)\n",
        "  acc1 = accuracy_score(val_labels,pred)\n",
        "  acc2 = neigh.score(val_data,val_labels)\n",
        "  print(acc1,acc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZxaGbhhPiP7",
        "outputId": "4e10ffbd-b1dd-40c6-9f33-da8be57de598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7 0.7\n",
            "0.5 0.5\n",
            "0.7 0.7\n",
            "0.3 0.3\n",
            "0.6 0.6\n"
          ]
        }
      ]
    }
  ]
}